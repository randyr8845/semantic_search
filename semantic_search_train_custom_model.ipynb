{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed666f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the code for training an embeddings model with custom data\n",
    "# This code was executed on an AWS Sagemaker notebook with GPUs (not my local laptop)\n",
    "# once the model was training, I compressed it can copied it to my local laptop\n",
    "# (I do not have GPUs for training on my local machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6382de8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install accelerate==1.3.0 #0.26.0\n",
    "# %pip install sentence-transformers==3.4.1\n",
    "# %pip install datasets==3.3.1\n",
    "\n",
    "\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import ast\n",
    "import copy\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, \\\n",
    "                                    SentenceTransformerTrainer, \\\n",
    "                                    SentenceTransformerTrainingArguments, \\\n",
    "                                    losses, \\\n",
    "                                    InputExample\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aada513",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"/home/ec2-user/SageMaker/QnA_model_train_test/QnA_train.csv\")\n",
    "\n",
    "print(\"Columns:\", train_data.columns)\n",
    "print(\"Number of recs:\", train_data.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f5ef86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test sets\n",
    "\n",
    "X_train, X_test = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "\n",
    "#save off test set for inference/model metrics\n",
    "X_test.to_csv(\"/home/ec2-user/SageMaker/QnA_model_train_test/test_set.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c34913",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a training set of question and answer pairs\n",
    "\n",
    "train_set = []\n",
    "for i,r in X_train.iterrows():\n",
    "    train_set.append(InputExample(texts=[str(r['answer']), str(r['question'])]))\n",
    "\n",
    "print(len(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c05d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the embeddings model to data set question and answer pairs \n",
    "\n",
    "# load the model\n",
    "#model = SentenceTransformer('sentence-transformers/msmarco-MiniLM-L6-cos-v5') # start from huggingface doanloaded pre-trained model\n",
    "model = SentenceTransformer(\"/home/ec2-user/SageMaker/QnA_model_train_test/models/final_model\") # continue training epoch on same mode\n",
    "\n",
    "#define the training set\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"anchor\": train_data['question'].astype(str).to_list(),\n",
    "    \"positive\": train_data['answer'].astype(str).to_list(),\n",
    "})\n",
    "\n",
    "# define the loss functon\n",
    "loss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "# set the training arguments\n",
    "model_args = SentenceTransformerTrainingArguments(\n",
    "    # Required parameter:\n",
    "    output_dir=\"models/trained_model_3\",\n",
    "    # Optional training parameters:\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=True,  # Set to False if your GPU can't handle FP16\n",
    "    bf16=False,  # Set to True if your GPU supports BF16\n",
    "    # Optional tracking/debugging parameters:\n",
    "    #eval_strategy=\"steps\",\n",
    "    #eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=500,\n",
    "    run_name=\"fine_tuned_embedding_model\"\n",
    ")\n",
    "\n",
    "#instanciate the trainer\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=model_args,\n",
    "    train_dataset=train_dataset,\n",
    "    loss=loss,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trained_model = trainer.train()\n",
    "\n",
    "# save the trained model\n",
    "trainer.save_model(\"/home/ec2-user/SageMaker/QnA_model_train_test/models/final_model\")\n",
    "\n",
    "#Starting Model Loss -->.106100\n",
    "#Ending Model Loss -->.005300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aff7a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the trained model\n",
    "loaded_model = SentenceTransformer('/home/ec2-user/SageMaker/QnA_model_train_test/models/final_model')\n",
    "\n",
    "# model test: compute some embeddings\n",
    "sentences = [\"This is an example sentence\", \"run fast through the woods my child\", \"Anchor 1\", 'Positive 1']\n",
    "embeddings = loaded_model.encode(sentences)\n",
    "print(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94430d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tar the model and send inference compute\n",
    "\n",
    "### tar -czf /home/ec2-user/SageMaker/QnA_model_train_test/models/custome_model.tar.gz /home/ec2-user/SageMaker/QnA_model_train_test/models/final_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
