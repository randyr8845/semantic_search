{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6f5597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the code for training an embeddings model with custom data\n",
    "# This code was executed on an AWS Sagemaker notebook with GPUs (not my local laptop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a994c9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install accelerate==1.3.0 #0.26.0\n",
    "# %pip install sentence-transformers==3.4.1\n",
    "# %pip install datasets==3.3.1\n",
    "\n",
    "\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import ast\n",
    "import copy\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, \\\n",
    "                                    SentenceTransformerTrainer, \\\n",
    "                                    SentenceTransformerTrainingArguments, \\\n",
    "                                    losses, \\\n",
    "                                    InputExample\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d13da96",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"/home/ec2-user/SageMaker/QnA_model_train_test/QnA_train.csv\")\n",
    "\n",
    "print(\"Columns:\", train_data.columns)\n",
    "print(\"Number of recs:\", train_data.shape[0])\n",
    "\n",
    "for i,r in \n",
    "train_data['question'].to_list()\n",
    "train_data[''].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dc6143",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a training set of question and answer pairs\n",
    "\n",
    "train_set = []\n",
    "for i,r in train_data.iterrows():\n",
    "    train_set.append(InputExample(texts=[str(r['answer']), str(r['question'])]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6cb08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the embeddings model to data set question and answer pairs \n",
    "\n",
    "# load the model\n",
    "#model = SentenceTransformer('sentence-transformers/msmarco-MiniLM-L6-cos-v5') # start from huggingface doanloaded pre-trained model\n",
    "model = SentenceTransformer(\"/home/ec2-user/SageMaker/QnA_model_train_test/models/final_model\") # continue training epoch on same mode\n",
    "\n",
    "#define the training set\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"anchor\": train_data['question'].astype(str).to_list(),\n",
    "    \"positive\": train_data['answer'].astype(str).to_list(),\n",
    "})\n",
    "\n",
    "# define the loss functon\n",
    "loss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "# set the training arguments\n",
    "model_args = SentenceTransformerTrainingArguments(\n",
    "    # Required parameter:\n",
    "    output_dir=\"models/trained_model_3\",\n",
    "    # Optional training parameters:\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=True,  # Set to False if your GPU can't handle FP16\n",
    "    bf16=False,  # Set to True if your GPU supports BF16\n",
    "    # Optional tracking/debugging parameters:\n",
    "    #eval_strategy=\"steps\",\n",
    "    #eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=500,\n",
    "    run_name=\"fine_tuned_embedding_model\"\n",
    ")\n",
    "\n",
    "#instanciate the trainer\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=model_args,\n",
    "    train_dataset=train_dataset,\n",
    "    loss=loss,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trained_model = trainer.train()\n",
    "\n",
    "# save the trained model\n",
    "trainer.save_model(\"/home/ec2-user/SageMaker/QnA_model_train_test/models/final_model\")\n",
    "\n",
    "#Starting Model Loss -->.535400\n",
    "#Ending Model Loss -->.005300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dfe28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the trained model\n",
    "loaded_model = SentenceTransformer('/home/ec2-user/SageMaker/QnA_model_train_test/models/final_model')\n",
    "\n",
    "# model test: compute some embeddings\n",
    "sentences = [\"This is an example sentence\", \"run fast through the woods my child\", \"Anchor 1\", 'Positive 1']\n",
    "embeddings = loaded_model.encode(sentences)\n",
    "print(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc7badc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tar the model and send inference compute\n",
    "\n",
    "### tar -czf /home/ec2-user/SageMaker/QnA_model_train_test/models/custome_model.tar.gz /home/ec2-user/SageMaker/QnA_model_train_test/models/final_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
